# Wandb Configuration

entity : "team"
project : "hogehoge"
name : "testjob"
group : "testgroup"

# Model Configuration

model : Llama2 # T5, GPT2, GPTNeoX, Llama2
n_dims : 64
n_heads : 16
n_layers : 27
intermediate_size : 2048

# Training Configuration

test_run : 10000
# block_size: 256
# logging_steps: 2
# num_train_epochs: 3
# save_total_limit: 5
# overwrite_output_dir: True
dp_lambda : 20



