import os
import time
import random
from pathlib import Path

import json
import hashlib
import subprocess
from urllib.parse import urlparse, parse_qs

from typing import List

# from collections import deque
# from filelock import FileLock

import numpy as np
import gzip
import pyzstd

from tqdm import tqdm

#import torch
from transformers import AutoTokenizer
from torch.utils.data import Dataset

from .commons import *

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 

def safe_dir(dir):
    if dir.endswith('/'):
        dir = dir[:-1]
    return dir

def safe_join_path(dir, file):
    if file is None: 
        return dir
    if dir.endswith('/'):
        dir = dir[:-1]
    if file.startswith('/'):
        file = file[1:]
    return f'{dir}/{file}'

def get_filebase(filename):
    filebase = filename
    if '/' in filebase:
        _, _, filebase = filebase.rpartition('/')
    filebase, _, _ = filebase.partition('.')
    return filebase

def get_filename_by_pid(prefix='cache'):
    return f'{prefix}{os.getpid()}'

## file 

def zopen(filepath):
    if filepath.endswith('.gz'):
        return gzip.open(filepath, 'rt')
    elif filepath.endswith('.zst'):
        return pyzstd.open(filepath, 'rt')
    else:
        return open(filepath, 'r')

def get_filelines(filepath):
    with zopen(filepath) as f:
        c=0
        line = f.readline()
        while line:
            c+=1
            line = f.readline()
    return c

def parse_strip(s):
    return s.strip().replace('<nL>', '\n')

def parse_jsonl(line):
    d = json.loads(line)
    if 'out' in d:
        return d['in'], d['out']
    return d['text']

def file_iterator(filename, N=None, args={}):
    if N == -1:
        N = get_filelines(filename)
    if N:
        from tqdm import tqdm
        pbar = tqdm(total=N, desc=filename)
    parse_fn = parse_strip
    if '.json' in filename:
        parse_fn = parse_jsonl
    c=0
    with zopen(filename) as f:
        line = f.readline()
        while line:
            c+=1
            if N: 
                pbar.update()
                if c > N: break
            yield parse_fn(line)
            line = f.readline()
    if N:
        pbar.close()

def parse_strip_nl(line):
    return line.strip().replace('<nL>', '\n')

def parse_static_text(line):
    d = json.loads(line)
    return d['text']

class parse_text:
    def __init__(self, key='text'):
        self.key = key
    def __call__(self, line):
        d = json.loads(line)
        return d[self.key]

class parse_seq2seq:
    def __init__(self, keyin, keyout):
        self.keyin = keyin
        self.keyout = keyout
    def __call__(self, line):
        d = json.loads(line)
        return d[self.keyin], d[self.keyout]


def detect_datatype(filename:str, args: dict):
    if '.json' in filename:
        with zopen(filename) as f:
            line = f.readline()
            d = json.loads(line)
            key = get_dict_multi_keys(args, 'column|columns|content', None)
            if key:
                keys = key.split(',')
                if len(keys) == 2 and keys[0] in d and keys[1] in d:
                    args['parse_fn'] = parse_seq2seq(keys[0], keys[1])
                    return 'seq2seq'
                if keys[0] in d:
                    args['parse_fn'] = parse_text(key[0])
                    return 'text'
            if 'in' in d and 'out' in d:
                args['parse_fn'] = parse_seq2seq('in', 'out')
                return 'seq2seq'
            if 'text' in d:
                args['parse_fn'] = parse_static_text
                return 'text'
            raise ValueError('ğŸ¦Š ã©ã®ãƒ‡ãƒ¼ã‚¿åˆ—ã‚’ä½¿ã†ã¤ã‚‚ã‚Šãªã®ã‹ãªï¼Ÿ', d)
    args['data_type'] = 'text'
    args['parse_fn'] = parse_strip_nl
    return args['data_type']

def iterate_line(filename, N=None, args={}):
    if N == -1:
        N = get_filelines(filename)
    if N:
        from tqdm import tqdm
        pbar = tqdm(total=N, desc=filename)
    parse_fn = args['parse_fn']
    c=0
    with zopen(filename) as f:
        line = f.readline()
        while line:
            c+=1
            if N: 
                pbar.update()
                if c > N: break
            yield parse_fn(line)
            line = f.readline()
    if N:
        pbar.close()

def _makedirs(path):
    dir, _,  file = path.rpartition("/")
    if '.' in file: #æ‹¡å¼µå­ãŒå«ã¾ã‚Œã‚‹å ´åˆ
        os.makedirs(dir,  exist_ok=True)
    elif not os.path.isfile(path):
        os.makedirs(path,  exist_ok=True)

def get_file_sha1(filepath: str):
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚¤ãƒŠãƒªãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã‚€
    with open(filepath, 'rb') as f:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
        content = f.read()
        # SHA-1ãƒãƒƒã‚·ãƒ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        sha1 = hashlib.sha1()
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¿½åŠ 
        sha1.update(content)
        # 16é€²æ•°ã§ãƒãƒƒã‚·ãƒ¥å€¤ã‚’å–å¾—
        sha1_hexdigest = sha1.hexdigest()
    return sha1_hexdigest

def get_filesize(file_path):
    if os.path.exists(file_path) and os.path.isfile(file_path):
        return os.path.getsize(file_path)
    else:
        return -1

def touch(file_path):
    file = Path(file_path)
    file.touch(exist_ok=True)

def wait_for_file(file_path, timeout=60):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ã‚’å®šæœŸçš„ã«ãƒã‚§ãƒƒã‚¯ã—ã€
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ True ã‚’è¿”ã—ã¾ã™ã€‚
    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸå ´åˆã¯ False ã‚’è¿”ã—ã¾ã™ã€‚
    """
    start_time = time.time()
    end_time = start_time + timeout
    while time.time() < end_time:
        if get_filesize(file_path) > 0:
            verbose_print(f'{time.time()-start_time} ç§’, å¾…ã¡ã¾ã—ãŸ')
            return True  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã£ãŸ
        time.sleep(0.5)  # 1ç§’å¾…ã¤
    return False  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ


def zstd_file(filename, rm=False, sync=True):
    if not os.path.exists(f'{filename}.zst'):
        if rm:
            cmd = f"zstd -q --rm {filename}"
        else:
            cmd = f"zstd -q {filename}"
        if not sync:
            cmd = f'{cmd} &'
        subprocess.call(cmd, shell=True, stderr=subprocess.DEVNULL)
    return f'{filename}.zst'


def unzstd_file(filename, rm=False, sync=True):
    if filename.endswith('.zst'):
        unzstd_filename = filename[:-4]
        if not not os.path.exists(unzstd_filename):
            if rm:
                cmd = f"zstd -dq --rm {filename}"
            else:
                cmd = f"zstd -dq {filename}"
            if not sync:
                cmd = f'{cmd} &'
            subprocess.call(cmd, shell=True, stderr=subprocess.DEVNULL)
            return unzstd_filename
    else:
        if not os.path.exists(filename) and os.path.exists(f'{filename}.zst'):
            return unzstd_file(f'{filename}.zst', rm=rm, sync=sync)
    return filename

def resolve_file(url_base, file_path, cache_dir, compressed=None, sync=True, verbose=True):
    remote_file = safe_join_path(url_base, file_path)
    # if remote_file.startswith('/'):
    #     # ãƒ­ãƒ¼ã‚«ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
    #     return remote_file
    cached_file = safe_join_path(cache_dir, file_path)
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œã£ã¦ãŠã
    os.makedirs(cached_file.rpartition("/")[0], exist_ok=True)
    cached_file_size = get_filesize(cached_file)
    if cached_file_size > 0:
        return cached_file
    
    if compressed:
        remote_file = f'{remote_file}.{compressed}'
        cached_file = f'{cached_file}.{compressed}'
        if os.path.exists(cached_file):
            return unzstd_file(cached_file)
    
    # ã‚³ãƒãƒ³ãƒ‰
    if remote_file.startswith('https://') or remote_file.startswith('http://'):
        cmd = f"wget -qO {cached_file}.tmp {remote_file} && mv {cached_file}.tmp {cached_file}"
    else:
        cmd = f'cp {remote_file} {cached_file}'

    if sync:
        if cached_file_size == 0:
            verbose_print('ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ æœ€å¤§30ç§’å¾…ã¡ã¾ã™.', remote_file)
            if wait_for_file(cached_file, 30):
                return cached_file
        touch(cached_file)
        subprocess.call(cmd, shell=True)
        cached_file_size = get_filesize(cached_file)
        if cached_file_size == 0:
            if verbose:
                verbose_print(f'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•— file={cached_file} {format_unit(cached_file_size, scale=1024)}B', cmd)
            os.remove(cached_file)
        else:
            if compressed:
                cached_file = unzstd_file(cached_file, rm=True)
            verbose_print(f'Downloaded {format_unit(cached_file_size, scale=1024)}B by', cmd)
        return cached_file

    if get_filesize(cached_file) == -1:
        touch(cached_file)
        verbose_print('ãƒ—ãƒ¬ãƒ•ã‚§ãƒƒãƒ', remote_file)
        if compressed:
            subprocess.call(f"{cmd} && zstd -dq --rm {cached_file} &", shell=True, stderr=subprocess.DEVNULL)
        else:
            subprocess.call(f"{cmd} &", shell=True, stderr=subprocess.DEVNULL)
    return None


# chunk file 

def chunkseq_to_filename(chunkseq:int, prefix:str, file_ext:str):
    dir = f"{(chunkseq//100):04d}"
    return safe_join_path(dir, f"{prefix}_{(chunkseq%100):02d}.{file_ext}")

def save_chunk_file(base_dir:str, chunk_file:str, chunks:List[np.ndarray]):
    filepath = safe_join_path(base_dir, chunk_file)
    _makedirs(filepath)
    if filepath.endswith('.npz'):
        np.savez(filepath, *chunks)
    # if filepath.endswith('.npz.zst'):
    #     filepath = filepath[:-4]
    #     np.savez(filepath, *chunks)
    #     zstd_file(filepath, rm=True, sync=False)
    # else:
    #     assert filepath.endswith('.npz')
    #     # np.savez_compressed(filepath, *chunks)

def load_chunk_file(base_dir:str, chunk_file:str=None, subblocks=1):
    if base_dir=='':
        filepath = chunk_file
    else:
        filepath = safe_join_path(base_dir, chunk_file)
    try:
        filepath = unzstd_file(filepath)
        npz = np.load(filepath)
        chunks = [npz[n] for n in npz.files]
        if subblocks > 1:
            newchunks=[]
            for chunk in chunks:
                splits = np.array_split(chunk, subblocks)
                newchunks.extend(splits)
            return newchunks
        return chunks
    except BaseException as e:
        verbose_print(f'ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ç ´æ {filepath}: åŸå›  {e}')
        return None

def check_chunk_file(base_dir:str, chunk_file:str, checks: dict):
    filepath = safe_join_path(base_dir, chunk_file)
    if 'filesize' in checks:
        if get_filesize(filepath) != checks['filesize']:
            return False
    if 'sha1' in checks:
        if get_file_sha1(filepath) != checks['sha1']:
            return False
    return True

def make_chunk_filelist(base_dir:str, chunk_files:List[str]):
    d = {}
    for chunk_file in tqdm(chunk_files, desc='File validation.'):
        if not load_chunk_file(base_dir, chunk_file):
            return None
        filepath = safe_join_path(base_dir, chunk_file)
        checks = {'filesize': get_filesize(filepath), 'sha1': get_file_sha1(filepath)}
        if not check_chunk_file(base_dir, chunk_file, checks):
            verbose_print(f'broken chunk file {chunk_file}')
            return None
        d[chunk_file] = checks
    return d

def shuffle_chunk_files(store_path: str, files:List[str], random_seed=42):
    for k in range(4):
        random.shuffle(files)
        for i in tqdm(range(0, len(files)-1, 2), desc=f'turn {k}'):
            chunks = load_chunk_file(store_path, files[i])
            chunks2 = load_chunk_file(store_path, files[i+1])
            length = len(chunks)
            merged_chunks = chunks+chunks2
            random.shuffle(merged_chunks)
            save_chunk_file(store_path, files[i], merged_chunks[:length])
            save_chunk_file(store_path, files[i+1], merged_chunks[length:])

# parse urls

def parse_url_list(url_list=[]):
    if isinstance(url_list, str):
        if os.path.exists(url_list):
            with open(url_list) as f:
                return [url.strip() for url in f.readlines() if url.strip() != '' and not url.startswith('#')]
        return url_list.split('|')
    return url_list

def _convert_to_number(value):
    """
    æ–‡å­—åˆ—ã‚’å¯èƒ½ãªã‚‰ã°æ•´æ•°ã¾ãŸã¯æµ®å‹•å°æ•°ç‚¹æ•°ã«å¤‰æ›ã™ã‚‹ã€‚
    å¤‰æ›ã§ããªã„å ´åˆã¯ãã®ã¾ã¾ã®æ–‡å­—åˆ—ã‚’è¿”ã™ã€‚
    """
    lower_string = str(value).lower()
    if lower_string == 'true':
        return True
    if lower_string == 'false':
        return False
    try:
        return int(value)
    except ValueError:
        try:
            return float(value)
        except ValueError:
            return str(value)

def parse_url_args(url, args={}):
    parsed_url = urlparse(url)
    param_args = parse_qs(parsed_url.query)
    param_args = {k: _convert_to_number(v[0]) for k, v in param_args.items()}
    param_args['url_scheme'] = parsed_url.scheme
    param_args['url_host'] = parsed_url.netloc
    param_args['url_path'] = parsed_url.path
    if len(parsed_url.scheme):
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}"
    else:
        base_url = f"{parsed_url.path}"
    args = args.copy()
    args.update(param_args)
    return base_url, args


def read_metadata(index_file_or_url, cache_dir=None):
    if cache_dir is not None:
        index_file = resolve_file(index_file_or_url, 'index.json', cache_dir)
    else:
        index_file = index_file_or_url
    if os.path.exists(index_file):
        with open(index_file, "r") as f:
            metadata = json.load(f)
            return metadata
    return {}

def write_metadata(index_file, metadata):
    with open(index_file, "w") as f:
        json.dump(metadata, f)

def find_better_prefix(metadata, args: dict):
    if 'prefix' in args:
        return args['prefix']
    prefixes = metadata.get('prefixes', None)
    if prefixes is None: # older version
        return 'pretrain' if args.get('data_type', 'text') else 'train'
    args_max_length = args['max_length']
    selected_prefix = None
    selected_max_length = 0
    for prefix, config in prefixes.items():
        if config['data_type'] == args['data_type']:
            max_length = config.get('max_length', DEFAULT_MAX_LENGTH)
            if max_length <= args_max_length and max_length > selected_max_length:
                selected_max_length = max_length
                selected_prefix = prefix
    return selected_prefix
            
def find_valid_prefix(metadata, train_prefix):
    prefixes = metadata.get('prefixes', {})
    if train_prefix.replace('train', 'valid') in prefixes:
        return train_prefix.replace('train', 'valid')
    if train_prefix.replace('train', 'dev') in prefixes:
        return train_prefix.replace('train', 'dev')
    return None
            


