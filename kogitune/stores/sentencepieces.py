import os
import kogitune.adhocs as adhoc
from kogitune.stores.files import list_filenames

def is_model_bpe(model_path):
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    with open(model_path, 'rb') as f:
        model_data = f.read()
    # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
    if b'type: BPE' in model_data:
        return True
    return False

def convert_fast_tokenizer(model_path, save_path='new_tokenizer'):
    import sentencepiece as spm
    from transformers import PreTrainedTokenizerFast
    if is_model_bpe(model_path):
        print('@BPE')
        from tokenizers import SentencePieceBPETokenizer
        # sentencepieceãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        tokenizer = SentencePieceBPETokenizer.from_file(model_path)
        # FastTokenizerã«å¤‰æ›
        fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)
    else:
        print('@Unigram')
        from transformers import T5Tokenizer
        fast_tokenizer = T5Tokenizer(model_path, extra_ids=0, additional_special_tokens=[])
    print(fast_tokenizer.get_vocab())
    fast_tokenizer.save_pretrained(save_path)

def generate_wakachi_file(files, output_file='temp'):
    try:
        from sudachipy import tokenizer
        from sudachipy import dictionary
    except ModuleNotFoundError as e:
        print(e)
        print('pip install sudachipy sudachidict_core')
        raise e

    # è¾æ›¸ã®æº–å‚™
    tokenizer_obj = dictionary.Dictionary().create()

    # åˆ†ã‹ã¡æ›¸ãã‚’è¡Œã†é–¢æ•°
    def wakachi(text):
        mode = tokenizer.Tokenizer.SplitMode.C  # åˆ†å‰²ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆA, B, CãŒã‚ã‚‹ï¼‰
        text = text[:12000]
        tokens = tokenizer_obj.tokenize(text, mode)
        return " ".join([t.surface() for t in tokens])

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§åˆ†ã‹ã¡æ›¸ãã—ã€çµæœã‚’æ›¸ãå‡ºã™
    with open(output_file, "w", encoding="utf-8") as fout:
        for file in files:
            with open(file, "r", encoding='utf-8') as fin:
                for line in fin:
                    wakachi_line = wakachi(line.strip())
                    fout.write(wakachi_line + "\n")
    return output_file

def generate_input_file(files, output_file):
    with open(output_file, "w", encoding="utf-8") as fout:
        for file in files:
            with open(file, "r", encoding='utf-8') as fin:
                for line in fin:
                    fout.write(line + "\n")
    return output_file

def remove_prefix_space(w, type=1, score=0.0):
    if type == 1:
        if w.startswith('â–') and len(w) > 1:
            return w[1:]
    return None

def replace_spm(model_path, replace_fn):
    os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'
    from sentencepiece import sentencepiece_model_pb2 as pb2
    m = pb2.ModelProto()
    with open(model_path, 'rb') as f:
        m.ParseFromString(f.read())
    voc = set(piece.piece for piece in m.pieces)
    for id, piece in enumerate(m.pieces):
        replaced = replace_fn(piece.piece, type=piece.type, score=piece.score)
        if replaced is not None:
            #print(id, piece.piece, '=>', replaced, replaced in voc)
            if replaced not in voc:
                piece.piece = replaced
    with open(model_path, 'wb') as f:
        f.write(m.SerializeToString())

def spm_train_cli(**kwargs):
    import sentencepiece as spm
    kwargs = kwargs | dict (
        #input='progtext_all.txt',  #data.txt
        model_prefix='spiece',  #ngmodel
        vocab_size=32000,
        # num_threads=72,
        # train_extremely_large_corpus=True,
        normalization_rule_name='identity',
        user_defined_symbols=['\n'],
        max_sentencepiece_length=8, # æ—¥æœ¬èªã¯æœ€å¤§é•·8
        split_digits=True,
        byte_fallback=True,
        split_by_whitespace=True, # ãƒ¢ãƒ‡ãƒ«ä½œæˆæ™‚ã¯ç©ºç™½ã§åŒºåˆ‡ã‚‹
        allow_whitespace_only_pieces=True,
        remove_extra_whitespaces=False,
        unk_id=2, pad_id=0, eos_id=1, bos_id=-1,
    )
    with adhoc.aargs_from(**kwargs) as aargs:
        files = list_filenames(aargs['files|!!'])
        use_wakachi = aargs['use_wakachi|=False']
        save_path = aargs['save_path']
        kwargs = adhoc.extract_kwargs(spm.SentencePieceTrainer.train, 
                                      aargs, 
                                      excludes=['save_path', 
                                                'files', 
                                                'use_wakachi', 
                                                'save_path'])
        input_file = f'input_{os.getpid()}.txt'
        remove_input_file = not os.path.exists(input_file)
        if use_wakachi:
            generate_wakachi_file(files, output_file=input_file)
        else:
            generate_input_file(files, output_file=input_file)
        kwargs['input'] = input_file
        prefix = kwargs['model_prefix']
        try:
            with adhoc.start_timer() as timer:
                adhoc.notice('SentencePieceãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´', options=kwargs)
                spm.SentencePieceTrainer.train(**kwargs)
                timer.notice('ãŠç–²ã‚Œæ§˜')
        except OSError as e:
            adhoc.notice('ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒå¤§', 
                         _read_here='https://github.com/google/sentencepiece/blob/master/doc/options.md')
            raise e
        finally:
            if remove_input_file:
                os.remove(input_file)

        if use_wakachi:
            replace_spm(f'{prefix}.model', remove_prefix_space)
        aargs.saved(f'{prefix}.model', f"SentencePieceãƒ¢ãƒ‡ãƒ« model_prefix='{prefix}'")
        aargs.saved(f'{prefix}.vocab', 'SentencePieceèªå½™')
        if save_path:
            convert_fast_tokenizer(f'{prefix}.model', save_path=save_path)
            aargs.saved(save_path, f"ç”Ÿæˆã•ã‚ŒãŸTokenizerã®ãƒ‘ã‚¹ save_path='{save_path}'")

def test_tokenizer(tokenizer):
    # ä½¿ç”¨ä¾‹
    text = """\
è‡ªç„¶è¨€èªå‡¦ç†ã¯é¢ç™½ã„æŠ€è¡“ã§ã™ã€‚
while True:
    a += 1 #ğŸ˜„
"""
    encoded = tokenizer.encode(text)
    print(len(encoded), encoded)
    decoded = tokenizer.decode(encoded)
    print(decoded)

def train_bpe_cli(**kwargs):
    from transformers import PreTrainedTokenizerFast
    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders

    with adhoc.aargs_from(**kwargs) as aargs:
        files = list_filenames(aargs['files|!!'])
        save_path = aargs['save_path']

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        tokenizer = Tokenizer(models.BPE())

        if aargs['bytelevel|byte_level|=True']:
            # ãƒ—ãƒ¬ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨­å®šï¼ˆæ–‡å­—å˜ä½ï¼‰
            tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

            # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š
            tokenizer.decoder = decoders.ByteLevel()
        else:
            # ãƒ—ãƒ¬ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ–‡å­—ãƒ¬ãƒ™ãƒ«ã«è¨­å®š
            tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
                pre_tokenizers.UnicodeScripts(),
                pre_tokenizers.Whitespace()
            ])

            # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š
            tokenizer.decoder = decoders.WordPiece(prefix="##")

            # # ãƒ—ãƒ¬ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ–‡å­—ãƒ¬ãƒ™ãƒ«ã«è¨­å®š
            # tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
            #     pre_tokenizers.UnicodeScripts(),
            #     pre_tokenizers.Metaspace(replacement=" ")#, add_prefix_space=False)
            # ])

            # # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š
            # tokenizer.decoder = decoders.Metaspace(replacement=" ")#, add_prefix_space=False)

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š
        trainer = trainers.BpeTrainer(
            vocab_size=aargs['vocab_size|=32000'],
            min_frequency=aargs['min_frequency|=2'],
            special_tokens=["[PAD]", 
                            "[UNK]", 
                            "[CLS]", 
                            "[SEP]", 
                            "[MASK]"]
        )

        with adhoc.start_timer() as timer:
            adhoc.notice('BPEãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’å§‹ã‚ã¾ã™', options=aargs)
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            tokenizer.train(files, trainer)
            timer.notice('ãŠç–²ã‚Œæ§˜ã™ã€‚')

        # FastTokenizerã®ä½œæˆ
        fast_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            unk_token="[UNK]",
            pad_token="[PAD]",
            cls_token="[CLS]",
            sep_token="[SEP]",
            mask_token="[MASK]",
        )
        adhoc.print('èªå½™', fast_tokenizer.get_vocab())
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜
        if save_path:
            fast_tokenizer.save_pretrained(save_path)
            aargs.saved(save_path, f"ä¿å­˜ã•ã‚ŒãŸTokenizerã®ãƒ‘ã‚¹ save_path='{save_path}'")

        test_tokenizer(fast_tokenizer)

def train_unigram_cli(**kwargs):
    from transformers import PreTrainedTokenizerFast
    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, decoders

    with adhoc.aargs_from(**kwargs) as aargs:
        files = list_filenames(aargs['files|!!'])
        save_path = aargs['save_path']

        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        tokenizer = Tokenizer(models.Unigram(byte_fallback=True))

        # ãƒ—ãƒ¬ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨­å®šï¼ˆæ–‡å­—å˜ä½ï¼‰
        tokenizer.pre_tokenizer = pre_tokenizers.UnicodeScripts()

        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š
        tokenizer.decoder = decoders.ByteLevel()

        # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®è¨­å®š
        trainer = trainers.UnigramTrainer(
            vocab_size=aargs['vocab_size|=32000'],
            n_sub_iterations=aargs['n_sub_iterations|=2'],
            max_piece_length=aargs['max_piece_length|=16'],
#            seed=aargs['seed|=42'],
# 
            unk_token="[UNK]",
            special_tokens=["[PAD]", 
                            "[UNK]", 
                            "[CLS]", 
                            "[SEP]", 
                            "[MASK]"]
        )

        tokenizer.post_processor = processors.TemplateProcessing(
            single="[CLS] $A [SEP]",
            pair="[CLS] $A [SEP] $B:1 [SEP]:1",
            special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
        )

        with adhoc.start_timer() as timer:
            adhoc.notice('UnigramModelãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´', options=aargs)
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            tokenizer.train(files, trainer)
            timer.notice('ãŠç–²ã‚Œæ§˜')

        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®è¨­å®š
        tokenizer.decoder = decoders.Metaspace()

        # FastTokenizerã®ä½œæˆ
        fast_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            unk_token="[UNK]",
            pad_token="[PAD]",
            cls_token="[CLS]",
            sep_token="[SEP]",
            mask_token="[MASK]",
        )
        print(fast_tokenizer.get_vocab())
        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜
        if save_path:
            fast_tokenizer.save_pretrained(save_path)
            aargs.saved(save_path, f"ä¿å­˜ã•ã‚ŒãŸTokenizerã®ãƒ‘ã‚¹ save_path='{save_path}'")

        test_tokenizer(fast_tokenizer)
